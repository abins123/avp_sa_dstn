{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40c0f8f-0c0e-4c86-bfc3-97674c76740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (4.39.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: datasets in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (3.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (0.22.1)\n",
      "Requirement already satisfied: packaging in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: evaluate in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (0.4.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (0.22.1)\n",
      "Requirement already satisfied: packaging in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: filelock in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /home/jovyan/conda/mistral-training2/lib/python3.12/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098f1d26-e71c-44b2-a484-92aedf9209e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_596/548544670.py:7: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  use_mps = True if torch.has_mps else False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "root_path = './'\n",
    "    \n",
    "use_mps = True if torch.has_mps else False\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8ad86-5778-4d7a-a6c7-3d34380bde8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f57f947-a621-4467-8361-2e4d2fc9a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, train_df_id=None, test_df_id=None, val_df_id=None, \n",
    "                 train_df_ood=None, test_df_ood=None, val_df_ood=None, sample_size = 1):\n",
    "        \n",
    "        self.train_df_id = train_df_id.sample(frac = sample_size, random_state = 1999) if train_df_id is not None else train_df_id\n",
    "        self.test_df_id = test_df_id\n",
    "        self.train_df_ood = train_df_ood\n",
    "        self.test_df_ood = test_df_ood\n",
    "        self.val_df_id = val_df_id\n",
    "        self.val_df_ood = val_df_ood\n",
    "\n",
    "    def reconstruct_strings(self, df, col):\n",
    "        \"\"\"\n",
    "        Reconstruct strings to dictionaries when loading csv/xlsx files.\n",
    "        \"\"\"\n",
    "        reconstructed_col = []\n",
    "        for text in df[col]:\n",
    "            if text != '[]' and isinstance(text, str):\n",
    "                text = text.replace('[', '').replace(']', '').replace('{', '').replace('}', '').split(\", '\")\n",
    "                req_list = []\n",
    "                for idx, pair in enumerate(text):\n",
    "                    splitter = ': ' if ': ' in pair else ':'\n",
    "                    if idx%2==0:\n",
    "                        reconstructed_dict = {} \n",
    "                        reconstructed_dict[pair.split(splitter)[0].replace(\"'\", '')] = pair.split(splitter)[1].replace(\"'\", '')\n",
    "                    else:\n",
    "                        reconstructed_dict[pair.split(splitter)[0].replace(\"'\", '')] = pair.split(splitter)[1].replace(\"'\", '')\n",
    "                        req_list.append(reconstructed_dict)\n",
    "            else:\n",
    "                req_list = text\n",
    "            reconstructed_col.append(req_list)\n",
    "        df[col] = reconstructed_col\n",
    "        return df\n",
    "\n",
    "    def extract_rowwise_aspect_polarity(self, df, on, key, min_val = None):\n",
    "        \"\"\"\n",
    "        Create duplicate records based on number of aspect term labels in the dataset.\n",
    "        Extract each aspect term for each row for reviews with muliple aspect term entries. \n",
    "        Do same for polarities and create new column for the same.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.iloc[0][on][0][key]\n",
    "        except:\n",
    "            df = self.reconstruct_strings(df, on)\n",
    "\n",
    "        df['len'] = df[on].apply(lambda x: len(x))\n",
    "        if min_val is not None:\n",
    "            df.loc[df['len'] == 0, 'len'] = min_val\n",
    "        df = df.loc[df.index.repeat(df['len'])]\n",
    "        df['record_idx'] = df.groupby(df.index).cumcount()\n",
    "        df['aspect'] = df[[on, 'record_idx']].apply(lambda x : (x[0][x[1]][key], x[0][x[1]]['polarity']) if len(x[0]) != 0 else ('',''), axis=1)\n",
    "        df['polarity'] = df['aspect'].apply(lambda x: x[-1])\n",
    "        df['aspect'] = df['aspect'].apply(lambda x: x[0])\n",
    "        df = df.drop(['len', 'record_idx'], axis=1).reset_index(drop = True)\n",
    "        return df\n",
    "    \n",
    "    def extract_rowwise_aspect_opinions(self, df, aspect_col, opinion_col, key, min_val = None):\n",
    "        \"\"\"\n",
    "        Create duplicate records based on number of aspect term labels in the dataset.\n",
    "        Extract each aspect term for each row for reviews with muliple aspect term entries. \n",
    "        Do same for polarities and create new column for the same.\n",
    "        \"\"\"\n",
    "        df['len'] = df[aspect_col].apply(lambda x: len(x))\n",
    "        if min_val is not None:\n",
    "            df.loc[df['len'] == 0, 'len'] = min_val\n",
    "        df = df.loc[df.index.repeat(df['len'])]\n",
    "        df['record_idx'] = df.groupby(df.index).cumcount()\n",
    "        df['aspect'] = df[[aspect_col, 'record_idx']].apply(lambda x : x[0][x[1]][key] if len(x[0]) != 0 else '', axis=1)\n",
    "        df['opinion_term'] = df[[opinion_col, 'record_idx']].apply(lambda x : x[0][x[1]][key] if len(x[0]) != 0 else '', axis=1)\n",
    "        df['aspect'] = df['aspect'].apply(lambda x: ' '.join(x))\n",
    "        df['opinion_term'] = df['opinion_term'].apply(lambda x: ' '.join(x))\n",
    "        df = df.drop(['len', 'record_idx'], axis=1).reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "    def create_data_in_ate_format(self, df, key, text_col, aspect_col, bos_instruction = '', \n",
    "                    eos_instruction = ''):\n",
    "        \"\"\"\n",
    "        Prepare the data in the input format required.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            return\n",
    "        try:\n",
    "            df.iloc[0][aspect_col][0][key]\n",
    "        except:\n",
    "            df = self.reconstruct_strings(df, aspect_col)\n",
    "        df['labels'] = df[aspect_col].apply(lambda x: ', '.join([i[key] for i in x]))\n",
    "        df['text'] = df[text_col].apply(lambda x: bos_instruction + x + eos_instruction)\n",
    "        return df\n",
    "\n",
    "    def create_data_in_atsc_format(self, df, on, key, text_col, aspect_col, bos_instruction = '', \n",
    "                    delim_instruction = '', eos_instruction = ''):\n",
    "        \"\"\"\n",
    "        Prepare the data in the input format required.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            return\n",
    "        try:\n",
    "            df.iloc[0][aspect_col][0][key]\n",
    "        except:\n",
    "            df = self.reconstruct_strings(df, aspect_col)\n",
    "        df['labels'] = df[aspect_col].apply(lambda x: ', '.join([i[key] for i in x]))\n",
    "        df['text'] = df[text_col].apply(lambda x: bos_instruction + x + eos_instruction)\n",
    "        return df\n",
    "\n",
    "    def create_data_in_aspe_format(self, df, key, label_key, text_col, aspect_col, bos_instruction = '', \n",
    "                                         eos_instruction = ''):\n",
    "        \"\"\"\n",
    "        Prepare the data in the input format required.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            return\n",
    "        try:\n",
    "            df.iloc[0][aspect_col][0][key]\n",
    "        except:\n",
    "            df = self.reconstruct_strings(df, aspect_col)\n",
    "        df['labels'] = df[aspect_col].apply(lambda x: ', '.join([f\"{i[key]}:{i[label_key]}\" for i in x]))\n",
    "        df['text'] = df[text_col].apply(lambda x: bos_instruction + x + eos_instruction)\n",
    "        return df\n",
    "    \n",
    "    def create_data_in_aooe_format(self, df, aspect_col, opinion_col, key, text_col, \n",
    "                               bos_instruction = '', delim_instruction = '', eos_instruction = ''):\n",
    "        \"\"\"\n",
    "        Prepare the data in the input format required.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            return\n",
    "        df = self.extract_rowwise_aspect_opinions(df, aspect_col=aspect_col, opinion_col=opinion_col, key=key, min_val=1)\n",
    "        df['text'] = df[[text_col, 'aspect']].apply(lambda x: bos_instruction + x[0] + delim_instruction + x[1] + eos_instruction, axis=1)\n",
    "        df = df.rename(columns = {'opinion_term': 'labels'})\n",
    "        return df\n",
    "    \n",
    "    def create_data_in_aope_format(self, df, key, text_col, aspect_col, opinion_col,\n",
    "                                         bos_instruction = '', eos_instruction = ''):\n",
    "        \"\"\"\n",
    "        Prepare the data in the input format required.\n",
    "        \"\"\"\n",
    "        df['labels'] = df[[aspect_col, opinion_col]].apply(lambda x: ', '.join([f\"{' '.join(i[key])}:{' '.join(j[key])}\" for i, j in zip(x[0], x[1])]), axis=1)\n",
    "        df['text'] = df[text_col].apply(lambda x: bos_instruction + x + eos_instruction)\n",
    "        return df\n",
    "    \n",
    "    def create_data_in_aoste_format(self, df, key, label_key, text_col, aspect_col, opinion_col,\n",
    "                                         bos_instruction = '', eos_instruction = ''):\n",
    "        \"\"\"\n",
    "        Prepare the data in the input format required.\n",
    "        \"\"\"\n",
    "        label_map = {'POS':'positive', 'NEG':'negative', 'NEU':'neutral'}\n",
    "        df['labels'] = df[[aspect_col, opinion_col]].apply(lambda x: ', '.join([f\"{' '.join(i[key])}:{' '.join(j[key])}:{label_map[i[label_key]]}\" for i, j in zip(x[0], x[1])]), axis=1)\n",
    "        df['text'] = df[text_col].apply(lambda x: bos_instruction + x + eos_instruction)\n",
    "        return df\n",
    "    \n",
    "    def set_data_for_training_semeval(self, tokenize_function):\n",
    "        \"\"\"\n",
    "        Create the training and test dataset as huggingface datasets format.\n",
    "        \"\"\"\n",
    "        # Define train and test sets\n",
    "        dataset_dict_id, dataset_dict_ood = {}, {}\n",
    "\n",
    "        if self.train_df_id is not None:\n",
    "            dataset_dict_id['train'] = Dataset.from_pandas(self.train_df_id)\n",
    "        if self.test_df_id is not None:\n",
    "            dataset_dict_id['test'] = Dataset.from_pandas(self.test_df_id)\n",
    "        if self.val_df_id is not None:\n",
    "            dataset_dict_id['validation'] = Dataset.from_pandas(self.val_df_id)\n",
    "        if len(dataset_dict_id) > 1:\n",
    "            indomain_dataset = DatasetDict(dataset_dict_id)\n",
    "            indomain_tokenized_datasets = indomain_dataset.map(tokenize_function, batched=True)\n",
    "        else:\n",
    "            indomain_dataset = {}\n",
    "            indomain_tokenized_datasets = {}\n",
    "\n",
    "        if self.train_df_ood is not None:\n",
    "            dataset_dict_ood['train'] = Dataset.from_pandas(self.train_df_ood)\n",
    "        if self.test_df_ood is not None:\n",
    "            dataset_dict_ood['test'] = Dataset.from_pandas(self.test_df_ood)\n",
    "        if self.val_df_ood is not None:\n",
    "            dataset_dict_ood['validation'] = Dataset.from_pandas(self.val_df_ood)\n",
    "        if len(dataset_dict_id) > 1:\n",
    "            other_domain_dataset = DatasetDict(dataset_dict_ood)\n",
    "            other_domain_tokenized_dataset = other_domain_dataset.map(tokenize_function, batched=True)\n",
    "        else:\n",
    "            other_domain_dataset = {}\n",
    "            other_domain_tokenized_dataset = {}\n",
    "\n",
    "        return indomain_dataset, indomain_tokenized_datasets, other_domain_dataset, other_domain_tokenized_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1898e80f-4fe1-49b6-b1d8-d546404aaa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 22:26:49.396703: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-16 22:26:49.410349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-16 22:26:49.426878: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-16 22:26:49.431962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-16 22:26:49.444018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-16 22:26:50.322643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq, AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments, Trainer, Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "\n",
    "class T5Generator:\n",
    "    def __init__(self, model_checkpoint):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer)\n",
    "        self.device = 'cuda' if torch.has_cuda else ('mps' if torch.has_mps else 'cpu')\n",
    "\n",
    "    def tokenize_function_inputs(self, sample):\n",
    "        \"\"\"\n",
    "        Udf to tokenize the input dataset.\n",
    "        \"\"\"\n",
    "        model_inputs = self.tokenizer(sample['text'], max_length=512, truncation=True)\n",
    "        labels = self.tokenizer(sample[\"labels\"], max_length=64, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "        \n",
    "    def train(self, tokenized_datasets, **kwargs):\n",
    "        \"\"\"\n",
    "        Train the generative model.\n",
    "        \"\"\"\n",
    "        #Set training arguments\n",
    "        args = Seq2SeqTrainingArguments(\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Define trainer object\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            self.model,\n",
    "            args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"] if tokenized_datasets.get(\"validation\") is not None else None,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "        )\n",
    "        print(\"Trainer device:\", trainer.args.device)\n",
    "\n",
    "        # Finetune the model\n",
    "        torch.cuda.empty_cache()\n",
    "        print('\\nModel training started ....')\n",
    "        trainer.train()\n",
    "\n",
    "        # Save best model\n",
    "        trainer.save_model()\n",
    "        return trainer\n",
    "\n",
    "    def get_labels(self, tokenized_dataset, batch_size = 4, max_length = 128, sample_set = 'train'):\n",
    "        \"\"\"\n",
    "        Get the predictions from the trained model.\n",
    "        \"\"\"\n",
    "        def collate_fn(batch):\n",
    "            input_ids = [torch.tensor(example['input_ids']) for example in batch]\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "            return input_ids\n",
    "        \n",
    "        dataloader = DataLoader(tokenized_dataset[sample_set], batch_size=batch_size, collate_fn=collate_fn)\n",
    "        predicted_output = []\n",
    "        self.model.to(self.device)\n",
    "        print('Model loaded to: ', self.device)\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch.to(self.device)\n",
    "            output_ids = self.model.generate(batch, max_length = max_length)\n",
    "            output_texts = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for output_text in output_texts:\n",
    "                predicted_output.append(output_text)\n",
    "        return predicted_output\n",
    "    \n",
    "    def get_metrics(self, y_true, y_pred, is_triplet_extraction=False):\n",
    "        total_pred = 0\n",
    "        total_gt = 0\n",
    "        tp = 0\n",
    "        if not is_triplet_extraction:\n",
    "            for gt, pred in zip(y_true, y_pred):\n",
    "                gt_list = gt.split(', ')\n",
    "                pred_list = pred.split(', ')\n",
    "                total_pred+=len(pred_list)\n",
    "                total_gt+=len(gt_list)\n",
    "                for gt_val in gt_list:\n",
    "                    for pred_val in pred_list:\n",
    "                        if pred_val in gt_val or gt_val in pred_val:\n",
    "                            tp+=1\n",
    "                            break\n",
    "\n",
    "        else:\n",
    "            for gt, pred in zip(y_true, y_pred):\n",
    "                gt_list = gt.split(', ')\n",
    "                pred_list = pred.split(', ')\n",
    "                total_pred+=len(pred_list)\n",
    "                total_gt+=len(gt_list)\n",
    "                for gt_val in gt_list:\n",
    "                    gt_asp = gt_val.split(':')[0]\n",
    "\n",
    "                    try:\n",
    "                        gt_op = gt_val.split(':')[1]\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        gt_sent = gt_val.split(':')[2]\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    for pred_val in pred_list:\n",
    "                        pr_asp = pred_val.split(':')[0]\n",
    "\n",
    "                        try:\n",
    "                            pr_op = pred_val.split(':')[1]\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            pr_sent = gt_val.split(':')[2]\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        if pr_asp in gt_asp and pr_op in gt_op and gt_sent == pr_sent:\n",
    "                            tp+=1\n",
    "\n",
    "        p = tp/total_pred\n",
    "        r = tp/total_gt\n",
    "        return p, r, 2*p*r/(p+r), None\n",
    "\n",
    "\n",
    "class T5Classifier:\n",
    "    def __init__(self, model_checkpoint):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, force_download = True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, force_download = True)\n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer)\n",
    "        self.device = 'cuda' if torch.has_cuda else ('mps' if torch.has_mps else 'cpu')\n",
    "\n",
    "    def tokenize_function_inputs(self, sample):\n",
    "        \"\"\"\n",
    "        Udf to tokenize the input dataset.\n",
    "        \"\"\"\n",
    "        sample['input_ids'] = self.tokenizer(sample[\"text\"], max_length = 512, truncation = True).input_ids\n",
    "        sample['labels'] = self.tokenizer(sample[\"labels\"], max_length = 64, truncation = True).input_ids\n",
    "        return sample\n",
    "        \n",
    "    def train(self, tokenized_datasets, **kwargs):\n",
    "        \"\"\"\n",
    "        Train the generative model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set training arguments\n",
    "        args = Seq2SeqTrainingArguments(\n",
    "            **kwargs\n",
    "            )\n",
    "\n",
    "        # Define trainer object\n",
    "        trainer = Trainer(\n",
    "            self.model,\n",
    "            args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"] if tokenized_datasets.get(\"validation\") is not None else None,\n",
    "            tokenizer=self.tokenizer, \n",
    "            data_collator = self.data_collator \n",
    "        )\n",
    "        print(\"Trainer device:\", trainer.args.device)\n",
    "\n",
    "        # Finetune the model\n",
    "        torch.cuda.empty_cache()\n",
    "        print('\\nModel training started ....')\n",
    "        trainer.train()\n",
    "\n",
    "        # Save best model\n",
    "        trainer.save_model()\n",
    "        return trainer\n",
    "\n",
    "    def get_labels(self, tokenized_dataset, batch_size = 4, sample_set = 'train'):\n",
    "        \"\"\"\n",
    "        Get the predictions from the trained model.\n",
    "        \"\"\"\n",
    "        def collate_fn(batch):\n",
    "            input_ids = [torch.tensor(example['input_ids']) for example in batch]\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "            return input_ids\n",
    "        \n",
    "        dataloader = DataLoader(tokenized_dataset[sample_set], batch_size=batch_size, collate_fn=collate_fn)\n",
    "        predicted_output = []\n",
    "        self.model.to(self.device)\n",
    "        print('Model loaded to: ', self.device)\n",
    "\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = batch.to(self.device)\n",
    "            output_ids = self.model.generate(batch)\n",
    "            output_texts = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for output_text in output_texts:\n",
    "                predicted_output.append(output_text)\n",
    "        return predicted_output\n",
    "    \n",
    "    def get_metrics(self, y_true, y_pred):\n",
    "        return precision_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='macro'), \\\n",
    "            f1_score(y_true, y_pred, average='macro'), accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ce854-9d70-406b-a260-40d67da4798e",
   "metadata": {},
   "source": [
    "## Instruction Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4152db2a-3c7c-4856-aa24-bb961a3f94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionsHandler:\n",
    "    def __init__(self):\n",
    "        self.ate = {}\n",
    "        self.atsc = {}\n",
    "        self.aspe = {}\n",
    "        self.aooe = {}\n",
    "        self.aope = {}\n",
    "        self.aoste = {}\n",
    "\n",
    "    def load_instruction_set1(self, ):\n",
    "\n",
    "        ################################# ATE #################################\n",
    "\n",
    "        self.ate['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) which have an associated opinion that are extracted from the input text. In cases where there are no aspects the output should be noaspectterm.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life\n",
    "        Positive example 2-\n",
    "        input: I even got my teenage son one, because of the features that it offers, like, iChat, Photobooth, garage band and more!.\n",
    "        output: features, iChat, Photobooth, garage band\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "\n",
    "        self.ate['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) which have an associated opinion that are extracted from the input text. In cases where there are no aspects the output should be noaspectterm.\n",
    "        Positive example 1-\n",
    "        input: With the great variety on the menu , I eat here often and never get bored.\n",
    "        output: menu\n",
    "        Positive example 2- \n",
    "        input: Great food, good size menu, great service and an unpretensious setting.\n",
    "        output: food, menu, service, setting\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.ate['delim_instruct'] = ''\n",
    "        self.ate['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "        ################################# ATSC #################################\n",
    "\n",
    "        self.atsc['bos_instruct1'] = \"\"\"Definition: The output will be 'positive' if the aspect identified in the sentence contains a positive sentiment. If the sentiment of the identified aspect in the input is negative the answer will be 'negative'. \n",
    "        Otherwise, the output should be 'neutral'. For aspects which are classified as noaspectterm, the sentiment is none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life. The aspect is battery life.\n",
    "        output: positive\n",
    "        Positive example 2-\n",
    "        input: I even got my teenage son one, because of the features that it offers, like, iChat, Photobooth, garage band and more!. The aspect is garage band.\n",
    "        output: positive\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.atsc['bos_instruct2'] = \"\"\"Definition: The output will be 'positive' if the aspect identified in the sentence contains a positive sentiment. If the sentiment of the identified aspect in the input is negative the answer will be 'negative'. \n",
    "        Otherwise, the output should be 'neutral'. For aspects which are classified as noaspectterm, the sentiment is none.\n",
    "        Positive example 1-\n",
    "        input: With the great variety on the menu , I eat here often and never get bored. The aspect is menu.\n",
    "        output: positive\n",
    "        Positive example 2- \n",
    "        input: Great food, good size menu, great service and an unpretensious setting. The aspect is food.\n",
    "        output: positive\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.atsc['delim_instruct'] = ' The aspect is '\n",
    "        self.atsc['eos_instruct'] = '.\\noutput:'\n",
    "\n",
    "        ################################# ASPE #################################\n",
    "\n",
    "        self.aspe['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the aspects sentiment polarity. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life:positive, \n",
    "        Positive example 2-\n",
    "        input: I even got my teenage son one, because of the features that it offers, like, iChat, Photobooth, garage band and more!.\n",
    "        output: features:positive, iChat:positive, Photobooth:positive, garage band:positive\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aspe['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the aspects sentiment polarity. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: With the great variety on the menu , I eat here often and never get bored.\n",
    "        output: menu:positive\n",
    "        Positive example 2- \n",
    "        input: Great food, good size menu, great service and an unpretensious setting.\n",
    "        output: food:positive, menu:positive, service:positive, setting:positive\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aspe['delim_instruct'] = ''\n",
    "        self.aspe['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "        ################################# AOOE #################################\n",
    "\n",
    "        self.aooe['bos_instruct1'] = \"\"\"Definition: The output will be the opinion/describing word of the aspect terms in the sentence. In cases where there are no aspects the output should be none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life . The aspect is battery life.\n",
    "        output: good\n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous. The aspect is GUI.\n",
    "        output: killer\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aooe['bos_instruct2'] = \"\"\"Definition: The output will be the opinion/describing word for the aspect term in the sentence. In cases where there are no aspects the output should be none.\n",
    "        Positive example 1-\n",
    "        input: Faan 's got a great concept but a little rough on the delivery . The aspect term is delivery.\n",
    "        output: rough\n",
    "        Positive example 2- \n",
    "        input: At the end you 're left with a mild broth with noodles that you can slurp out of a cup . The aspect term is broth with noodles.\n",
    "        output: mild\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aooe['delim_instruct'] = ' The aspect is '\n",
    "        self.aooe['eos_instruct'] = '.\\noutput:'\n",
    "\n",
    "        ################################# AOPE #################################\n",
    "\n",
    "        self.aope['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the corresponding opinion/describing terms. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life:good \n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous.\n",
    "        output: quality:high, GUI:killer, applications:good, use:easy \n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aope['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the aspects sentiment polarity. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: Faan 's got a great concept but a little rough on the delivery .\n",
    "        output: delivery:rough\n",
    "        Positive example 2- \n",
    "        input: I just wonder how you can have such a delicious meal for such little money .\n",
    "        output: meal:delicious, money:little\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aope['delim_instruct'] = ''\n",
    "        self.aope['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "        ################################# AOSTE #################################\n",
    "\n",
    "        self.aoste['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) the corresponding opinion/describing terms and the sentiment polarity (positive, negative, neutral) of the opinion term . In cases where there are no aspects the output should be noaspectterm:none:none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life:good:positive \n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous.\n",
    "        output: quality:high:positive, GUI:killer:positive, applications:good:positive, use:easy:positive \n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aoste['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) the corresponding opinion/describing terms and the sentiment polarity (positive, negative, neutral) of the opinion term . In cases where there are no aspects the output should be noaspectterm:none:none.\n",
    "        Positive example 1-\n",
    "        input: Faan 's got a great concept but a little rough on the delivery .\n",
    "        output: delivery:rough:positive\n",
    "        Positive example 2- \n",
    "        input: I just wonder how you can have such a delicious meal for such little money .\n",
    "        output: meal:delicious:positive, money:little:positive\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aoste['delim_instruct'] = ''\n",
    "        self.aoste['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "\n",
    "    def load_instruction_set2(self, ):\n",
    "\n",
    "        ################################# ATE #################################\n",
    "\n",
    "        self.ate['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) which have an associated opinion that are extracted from the input text. In cases where there are no aspects the output should be noaspectterm.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life\n",
    "        Positive example 2-\n",
    "        input: I even got my teenage son one, because of the features that it offers, like, iChat, Photobooth, garage band and more!.\n",
    "        output: features, iChat, Photobooth, garage band\n",
    "        Negative example 1-\n",
    "        input: Speaking of the browser, it too has problems.\n",
    "        output: browser\n",
    "        Negative example 2-\n",
    "        input: The keyboard is too slick.\n",
    "        output: keyboard\n",
    "        Neutral example 1-\n",
    "        input: I took it back for an Asus and same thing- blue screen which required me to remove the battery to reset.\n",
    "        output: battery\n",
    "        Neutral example 2-\n",
    "        input: Nightly my computer defrags itself and runs a virus scan.\n",
    "        output: virus scan\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.ate['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) which have an associated opinion that are extracted from the input text. In cases where there are no aspects the output should be noaspectterm.\n",
    "        Positive example 1-\n",
    "        input: With the great variety on the menu , I eat here often and never get bored.\n",
    "        output: menu\n",
    "        Positive example 2- \n",
    "        input: Great food, good size menu, great service and an unpretensious setting.\n",
    "        output: food, menu, service, setting\n",
    "        Negative example 1-\n",
    "        input: They did not have mayonnaise, forgot our toast, left out ingredients (ie cheese in an omelet), below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it.\n",
    "        output: toast, mayonnaise, bacon, ingredients, plate\n",
    "        Negative example 2-\n",
    "        input: The seats are uncomfortable if you are sitting against the wall on wooden benches.\n",
    "        output: seats\n",
    "        Neutral example 1-\n",
    "        input: I asked for seltzer with lime, no ice.\n",
    "        output: seltzer with lime\n",
    "        Neutral example 2-\n",
    "        input: They wouldnt even let me finish my glass of wine before offering another.\n",
    "        output: glass of wine\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.ate['delim_instruct'] = ''\n",
    "        self.ate['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "        ################################# ATSC #################################\n",
    "\n",
    "        self.atsc['bos_instruct1'] = \"\"\"Definition: The output will be 'positive' if the aspect identified in the sentence contains a positive sentiment. If the sentiment of the identified aspect in the input is negative the answer will be 'negative'. \n",
    "        Otherwise, the output should be 'neutral'. For aspects which are classified as noaspectterm, the sentiment is none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life. The aspect is battery life.\n",
    "        output: positive\n",
    "        Positive example 2-\n",
    "        input: I even got my teenage son one, because of the features that it offers, like, iChat, Photobooth, garage band and more!. The aspect is garage band.\n",
    "        output: positive\n",
    "        Negative example 1-\n",
    "        input: Speaking of the browser, it too has problems. The aspect is browser.\n",
    "        output: negative\n",
    "        Negative example 2-\n",
    "        input: The keyboard is too slick. The aspect is keyboard.\n",
    "        output: negative\n",
    "        Neutral example 1-\n",
    "        input: I took it back for an Asus and same thing- blue screen which required me to remove the battery to reset. The aspect is battery.\n",
    "        output: neutral\n",
    "        Neutral example 2-\n",
    "        input: Nightly my computer defrags itself and runs a virus scan. The aspect is virus scan.\n",
    "        output: neutral\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.atsc['bos_instruct2'] = \"\"\"Definition: The output will be 'positive' if the aspect identified in the sentence contains a positive sentiment. If the sentiment of the identified aspect in the input is negative the answer will be 'negative'. \n",
    "        Otherwise, the output should be 'neutral'. For aspects which are classified as noaspectterm, the sentiment is none.\n",
    "        Positive example 1-\n",
    "        input: With the great variety on the menu , I eat here often and never get bored. The aspect is menu.\n",
    "        output: positive\n",
    "        Positive example 2- \n",
    "        input: Great food, good size menu, great service and an unpretensious setting. The aspect is food.\n",
    "        output: positive\n",
    "        Negative example 1-\n",
    "        input: They did not have mayonnaise, forgot our toast, left out ingredients (ie cheese in an omelet), below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it. The aspect is toast.\n",
    "        output: negative\n",
    "        Negative example 2-\n",
    "        input: The seats are uncomfortable if you are sitting against the wall on wooden benches. The aspect is seats.\n",
    "        output: negative\n",
    "        Neutral example 1-\n",
    "        input: I asked for seltzer with lime, no ice. The aspect is seltzer with lime.\n",
    "        output: neutral\n",
    "        Neutral example 2-\n",
    "        input: They wouldnt even let me finish my glass of wine before offering another. The aspect is glass of wine.\n",
    "        output: neutral\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.atsc['delim_instruct'] = ' The aspect is '\n",
    "        self.atsc['eos_instruct'] = '.\\noutput:'\n",
    "\n",
    "        ################################# ASPE #################################\n",
    "\n",
    "        self.aspe['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the aspects sentiment polarity. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life:positive, \n",
    "        Positive example 2-\n",
    "        input: I even got my teenage son one, because of the features that it offers, like, iChat, Photobooth, garage band and more!.\n",
    "        output: features:positive, iChat:positive, Photobooth:positive, garage band:positive\n",
    "        Negative example 1-\n",
    "        input: Speaking of the browser, it too has problems.\n",
    "        output: browser:negative\n",
    "        Negative example 2-\n",
    "        input: The keyboard is too slick.\n",
    "        output: keyboard:negative\n",
    "        Neutral example 1-\n",
    "        input: I took it back for an Asus and same thing- blue screen which required me to remove the battery to reset.\n",
    "        output: battery:neutral\n",
    "        Neutral example 2-\n",
    "        input: Nightly my computer defrags itself and runs a virus scan.\n",
    "        output: virus scan:neutral\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aspe['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the aspects sentiment polarity. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: With the great variety on the menu , I eat here often and never get bored.\n",
    "        output: menu:positive\n",
    "        Positive example 2- \n",
    "        input: Great food, good size menu, great service and an unpretensious setting.\n",
    "        output: food:positive, menu:positive, service:positive, setting:positive\n",
    "        Negative example 1-\n",
    "        input: They did not have mayonnaise, forgot our toast, left out ingredients (ie cheese in an omelet), below hot temperatures and the bacon was so over cooked it crumbled on the plate when you touched it.\n",
    "        output: toast:negative, mayonnaise:negative, bacon:negative, ingredients:negative, plate:negative\n",
    "        Negative example 2-\n",
    "        input: The seats are uncomfortable if you are sitting against the wall on wooden benches.\n",
    "        output: seats:negative\n",
    "        Neutral example 1-\n",
    "        input: I asked for seltzer with lime, no ice.\n",
    "        output: seltzer with lime:neutral\n",
    "        Neutral example 2-\n",
    "        input: They wouldnt even let me finish my glass of wine before offering another.\n",
    "        output: glass of wine:neutral\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aspe['delim_instruct'] = ''\n",
    "        self.aspe['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "        ################################# AOOE #################################\n",
    "\n",
    "        self.aooe['bos_instruct1'] = \"\"\"Definition: The output will be the opinion/describing word of the aspect terms in the sentence. In cases where there are no aspects the output should be none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life . The aspect is battery life.\n",
    "        output: good\n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous. The aspect is GUI.\n",
    "        output: killer\n",
    "        Negative example 1-\n",
    "        input: One night I turned the freaking thing off after using it , the next day I turn it on , no GUI , screen all dark , power light steady , hard drive light steady and not flashing as it usually does . The aspect is GUI.\n",
    "        output: no\n",
    "        Negative example 2-\n",
    "        input: I can barely use any usb devices because they will not stay connected properly . The aspect is usb devices.\n",
    "        output: not stay connected properly\n",
    "        Neutral example 1-\n",
    "        input: However , the multi-touch gestures and large tracking area make having an external mouse unnecessary ( unless you 're gaming ) . The aspect is external mouse.\n",
    "        output: unnecessary\n",
    "        Neutral example 2-\n",
    "        input: I wanted to purchase the extended warranty and they refused , because they knew it was trouble . The aspect is extended warranty.\n",
    "        output: refused\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "\n",
    "        self.aooe['bos_instruct2'] = \"\"\"Definition: The output will be the opinion/describing word of the aspect terms in the sentence. In cases where there are no aspects the output should be none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life . The aspect is battery life.\n",
    "        output: good\n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous. The aspect is GUI.\n",
    "        output: killer\n",
    "        Negative example 1-\n",
    "        input: The menu is very limited - i think we counted 4 or 5 entrees . The aspect is menu.\n",
    "        output: limited\n",
    "        Negative example 2-\n",
    "        input: The strong scents coming from the left and right of me negatively affected my taste buds . The aspect is scents.\n",
    "        output: strong\n",
    "        Neutral example 1-\n",
    "        input: What came to our table was burned beyond recognition and stringy . The aspect is battery table.\n",
    "        output: burned\n",
    "        Neutral example 2-\n",
    "        input: But , nothing stands out about the cooking . The aspect is cooking.\n",
    "        output: nothing stands out\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aooe['delim_instruct'] = ' The aspect is '\n",
    "        self.aooe['eos_instruct'] = '.\\noutput:'\n",
    "\n",
    "        ################################# AOPE #################################\n",
    "\n",
    "        self.aope['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the corresponding opinion/describing terms. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life:good \n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous.\n",
    "        output: quality:high, GUI:killer, applications:good, use:easy\n",
    "        Negative example 1-\n",
    "        input: A month or so ago , the freaking motherboard just died .\n",
    "        output: motherboard:freaking, motherboard:freaking\n",
    "        Negative example 2-\n",
    "        input: I had always used PCs and been constantly frustrated by the crashing and the poorly designed operating systems that were never very intuitive .\n",
    "        output: operating systems:poorly designed, operating systems:intuitive\n",
    "        Neutral example 1-\n",
    "        input: It has a 10 hour battery life when you 're doing web browsing and word editing , making it perfect for the classroom or office , and in terms of gaming and movie playing it 'll have a battery life of just over 5 hours .\n",
    "        output: web browsing:perfect, word editing:perfect\n",
    "        Neutral example 2-\n",
    "        input: no complaints with their desktop , and maybe because it just sits on your desktop , and you do n't carry it around , which could jar the hard drive , or the motherboard .\n",
    "        output: hard drive:jar, motherboard:jar\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aope['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) and the aspects sentiment polarity. In cases where there are no aspects the output should be noaspectterm:none.\n",
    "        Positive example 1-\n",
    "        input: Faan 's got a great concept but a little rough on the delivery .\n",
    "        output: delivery:rough\n",
    "        Positive example 2- \n",
    "        input: I just wonder how you can have such a delicious meal for such little money .\n",
    "        output: meal:delicious, money:little\n",
    "        Negative example 1-\n",
    "        input: From the terrible service , to the bland food , not to mention the unaccommodating managers , the overall experience was horrible .\n",
    "        output: service:terrible, food:bland, managers:unaccommodating\n",
    "        Negative example 2- \n",
    "        input: I had the Pad Thai and the noodles were sticky .\n",
    "        output: Pad Thai:sticky, noodles:sticky\n",
    "        Neutral example 1-\n",
    "        input: The Dim Sum was so-so , but not spectacular .\n",
    "        output: Dim Sum:so-so, Dim Sum:not spectacular\n",
    "        Neutral example 2- \n",
    "        input: The location and ambience is Ok but the food is what makes up for it .\n",
    "        output: mlocationeal:Ok, ambience:Ok\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aope['delim_instruct'] = ''\n",
    "        self.aope['eos_instruct'] = ' \\noutput:'\n",
    "\n",
    "        ################################# AOSTE #################################\n",
    "\n",
    "        self.aoste['bos_instruct1'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) the corresponding opinion/describing terms and the sentiment polarity (positive, negative, neutral) of the opinion term . In cases where there are no aspects the output should be noaspectterm:none:none.\n",
    "        Positive example 1-\n",
    "        input: I charge it at night and skip taking the cord with me because of the good battery life.\n",
    "        output: battery life:good:positive \n",
    "        Positive example 2-\n",
    "        input: it is of high quality , has a killer GUI , is extremely stable , is highly expandable , is bundled with lots of very good applications , is easy to use , and is absolutely gorgeous.\n",
    "        output: quality:high:positive, GUI:killer:positive, applications:good:positive, use:easy:positive\n",
    "        Negative example 1-\n",
    "        input: A month or so ago , the freaking motherboard just died .\n",
    "        output: motherboard:freaking:negative, motherboard:freaking:negative\n",
    "        Negative example 2-\n",
    "        input: I had always used PCs and been constantly frustrated by the crashing and the poorly designed operating systems that were never very intuitive .\n",
    "        output: operating systems:poorly designed, operating systems:intuitive\n",
    "        Neutral example 1-\n",
    "        input: It has a 10 hour battery life when you 're doing web browsing and word editing , making it perfect for the classroom or office , and in terms of gaming and movie playing it 'll have a battery life of just over 5 hours .\n",
    "        output: web browsing:perfect:neutral, word editing:perfect:neutral\n",
    "        Neutral example 2-\n",
    "        input: no complaints with their desktop , and maybe because it just sits on your desktop , and you do n't carry it around , which could jar the hard drive , or the motherboard .\n",
    "        output: hard drive:jar:neutral, motherboard:jar:neutral\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        \n",
    "        self.aoste['bos_instruct2'] = \"\"\"Definition: The output will be the aspects (both implicit and explicit) the corresponding opinion/describing terms and the sentiment polarity (positive, negative, neutral) of the opinion term . In cases where there are no aspects the output should be noaspectterm:none:none.\n",
    "        Positive example 1-\n",
    "        input: Faan 's got a great concept but a little rough on the delivery .\n",
    "        output: delivery:rough:positive\n",
    "        Positive example 2- \n",
    "        input: I just wonder how you can have such a delicious meal for such little money .\n",
    "        output: meal:delicious:positive, money:little:positive\n",
    "        Negative example 1-\n",
    "        input: From the terrible service , to the bland food , not to mention the unaccommodating managers , the overall experience was horrible .\n",
    "        output: service:terrible:negative, food:bland:negative, managers:unaccommodating:negative\n",
    "        Negative example 2- \n",
    "        input: I had the Pad Thai and the noodles were sticky .\n",
    "        output: Pad Thai:sticky:negative, noodles:sticky:negative\n",
    "        Neutral example 1-\n",
    "        input: The Dim Sum was so-so , but not spectacular .\n",
    "        output: Dim Sum:so-so:neutral, Dim Sum:not spectacular:neutral\n",
    "        Neutral example 2- \n",
    "        input: The location and ambience is Ok but the food is what makes up for it .\n",
    "        output: mlocationeal:Ok:neutral, ambience:Ok:neutral\n",
    "        Now complete the following example-\n",
    "        input: \"\"\"\n",
    "        self.aoste['delim_instruct'] = ''\n",
    "        self.aoste['eos_instruct'] = ' \\noutput:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e604000-1b41-464a-bfac-24fd30ce03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de905abb-a252-4016-a0a5-316acb53eba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name:  LaptopData\n",
      "Model output path:  ./Models/AspectTermAndSentimentAnalysis/allenaitk-instruct-base-def-pos-LaptopData\n"
     ]
    }
   ],
   "source": [
    "task_name = 'AspectTermAndSentimentAnalysis'\n",
    "experiment_name = 'LaptopData'\n",
    "model_checkpoint = 'allenai/tk-instruct-base-def-pos'\n",
    "print('Experiment Name: ', experiment_name)\n",
    "model_out_path = './Models'\n",
    "model_out_path = os.path.join(model_out_path, task_name, f\"{model_checkpoint.replace('/', '')}-{experiment_name}\")\n",
    "print('Model output path: ', model_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de699a2a-7019-4a13-a666-a1d7dab311f5",
   "metadata": {},
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a13bb5-0ed5-4ff3-af3e-aa43762930c3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a723a007-fab1-4ae2-a384-8e8c5bf4344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/SemEval14/Train/Laptops_Train.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/SemEval14/Test/Laptops_Test.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set1()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "290d5448-ee39-4c40-bb0e-709dca381cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 3045/3045 [00:00<00:00, 9411.26 examples/s]\n",
      "Map: 100%|| 800/800 [00:00<00:00, 8795.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create T5 utils object\n",
    "t5_exp = T5Generator(model_checkpoint)\n",
    "\n",
    "# Tokenize Dataset\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenized_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Training arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3987d2-165a-4f01-9ba8-5b3a2860c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'output_dir':model_out_path,\n",
    "    'evaluation_strategy':\"no\",\n",
    "    'learning_rate':5e-5,\n",
    "    'lr_scheduler_type':'cosine',\n",
    "    'per_device_train_batch_size':8,\n",
    "    'per_device_eval_batch_size':16,\n",
    "    'num_train_epochs':4,\n",
    "    'weight_decay':0.01,\n",
    "    'warmup_ratio':0.1,\n",
    "    'save_strategy':'no',\n",
    "    'load_best_model_at_end':False,\n",
    "    'push_to_hub':False,\n",
    "    'eval_accumulation_steps':1,\n",
    "    'predict_with_generate':True,\n",
    "    'use_mps_device':use_mps,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "716bc1b4-95d7-44d3-b19b-1e604bc881c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdc6b7cc-38a0-4515-8fbb-1044edd976a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer device: cuda:0\n",
      "\n",
      "Model training started ....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 03:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "model_trainer = t5_exp.train(id_tokenized_ds, **training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa2190b-b7f4-4808-8f3a-c77613555734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.save_model('./NewModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d67b04-f34f-4e3a-b213-9f380fce8236",
   "metadata": {},
   "source": [
    "## Train ATSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5436a202-2362-497b-ae4e-a9398f544a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/SemEval14/Train/Laptops_Train.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/SemEval14/Test/Laptops_Test.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "instruct_handler.load_instruction_set1()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33f83341-ad02-4436-8303-9fd583dace7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 3045/3045 [00:00<00:00, 8953.99 examples/s]\n",
      "Map: 100%|| 800/800 [00:00<00:00, 8488.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create T5 utils object\n",
    "t5_exp_classify = T5Generator(model_checkpoint)\n",
    "\n",
    "# Tokenize Dataset\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenized_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Training arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04fe3569-a1af-40d2-9dd5-52badf6d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'output_dir':'/ATSC/',\n",
    "    'evaluation_strategy':\"no\",\n",
    "    'learning_rate':25e-5,\n",
    "    'lr_scheduler_type':'cosine',\n",
    "    'per_device_train_batch_size':8,\n",
    "    'per_device_eval_batch_size':16,\n",
    "    'num_train_epochs':4,\n",
    "    'weight_decay':0.06,\n",
    "    'warmup_ratio':0.3,\n",
    "    'save_strategy':'no',\n",
    "    'load_best_model_at_end':False,\n",
    "    'push_to_hub':False,\n",
    "    'eval_accumulation_steps':1,\n",
    "    'predict_with_generate':True,\n",
    "    'use_mps_device':use_mps,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7e82a62-7231-4651-9d5a-3918f17753a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer device: cuda:0\n",
      "\n",
      "Model training started ....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 03:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "model_trainer = t5_exp_classify.train(id_tokenized_ds, **training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1600e3-bcaa-4dc1-99a5-2a609668bb6b",
   "metadata": {},
   "source": [
    "## Inference ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10744381-9297-439e-9692-e163d1393618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/SemEval14/Train/Laptops_Train.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/SemEval14/Test/Laptops_Test.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "\n",
    "instruct_handler.load_instruction_set1()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "787e55ee-c0fa-4f78-a062-03a46db33189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 3045/3045 [00:00<00:00, 8487.99 examples/s]\n",
      "Map: 100%|| 800/800 [00:00<00:00, 8739.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 191/191 [00:35<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Model inference - Loading from Checkpoint\n",
    "t5_exp = T5Generator(model_out_path)\n",
    "\n",
    "# Tokenize Datasets\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Get prediction labels - Training set   \n",
    "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16)\n",
    "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
    "\n",
    "# Get prediction labels - Testing set\n",
    "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test',  batch_size = 16)\n",
    "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c31e83e3-3794-4f0d-83e7-df9f52c97d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision:  0.9084633086167221\n",
      "Train Recall:  0.9075351213282248\n",
      "Train F1:  0.9079989777664197\n",
      "Test Precision:  0.923837784371909\n",
      "Test Recall:  0.9050387596899225\n",
      "Test F1:  0.9143416544297602\n"
     ]
    }
   ],
   "source": [
    "p, r, f1, x = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
    "print('Train Precision: ', p)\n",
    "print('Train Recall: ', r)\n",
    "print('Train F1: ', f1)\n",
    "\n",
    "p, r, f1, x = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
    "print('Test Precision: ', p)\n",
    "print('Test Recall: ', r)\n",
    "print('Test F1: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05065a7-54fd-4fec-b5c2-55cbfa16e0d9",
   "metadata": {},
   "source": [
    "## INFERENCE and EVAL ATSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ea5084b-6a01-43b8-b799-f486321e1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "id_train_file_path = './Dataset/SemEval14/SemEval14/Train/Laptops_Train.csv'\n",
    "id_test_file_path = './Dataset/SemEval14/SemEval14/Test/Laptops_Test.csv'\n",
    "id_tr_df = pd.read_csv(id_train_file_path)\n",
    "id_te_df = pd.read_csv(id_test_file_path)\n",
    "\n",
    "# Get the input text into the required format using Instructions\n",
    "instruct_handler = InstructionsHandler()\n",
    "\n",
    "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
    "\n",
    "instruct_handler.load_instruction_set1()\n",
    "\n",
    "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
    "loader = DatasetLoader(id_tr_df, id_te_df)\n",
    "if loader.train_df_id is not None:\n",
    "    loader.train_df_id = loader.create_data_in_atsc_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
    "if loader.test_df_id is not None:\n",
    "    loader.test_df_id = loader.create_data_in_atsc_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b56cb86d-4abd-4a86-aed4-077c5d999fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 3045/3045 [00:00<00:00, 8441.65 examples/s]\n",
      "Map: 100%|| 800/800 [00:00<00:00, 8580.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 191/191 [00:34<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Model inference - Loading from Checkpoint\n",
    "t5_exp = T5Generator(model_out_path)\n",
    "\n",
    "# Tokenize Datasets\n",
    "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
    "\n",
    "# Get prediction labels - Training set   \n",
    "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16)\n",
    "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
    "\n",
    "# Get prediction labels - Testing set\n",
    "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test',  batch_size = 16)\n",
    "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c2c3f02-0b4c-4ae6-bbb6-fa86e94956a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision:  0.9628776241679468\n",
      "Train Recall:  0.9606641123882503\n",
      "Train F1:  0.9617695946809871\n",
      "Test Precision:  0.9477317554240631\n",
      "Test Recall:  0.9312015503875969\n",
      "Test F1:  0.9393939393939394\n"
     ]
    }
   ],
   "source": [
    "p, r, f1, x = t5_exp_classify.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
    "print('Train Precision: ', p)\n",
    "print('Train Recall: ', r)\n",
    "print('Train F1: ', f1)\n",
    "\n",
    "p, r, f1, x = t5_exp_classify.get_metrics(id_te_labels, id_te_pred_labels)\n",
    "print('Test Precision: ', p)\n",
    "print('Test Recall: ', r)\n",
    "print('Test F1: ', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mistral-training2]",
   "language": "python",
   "name": "conda-env-mistral-training2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
